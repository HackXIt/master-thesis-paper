@article{ahaProceedings2005IJCAI,
  title = {Proceedings of the 2005 {{IJCAI Workshop}} on {{Reasoning}}, {{Representation}}, and {{Learning}} in {{Computer Games}}},
  author = {Aha, David W and Mu√±oz-Avila, H√©ctor and family=Lent, given=Michael, prefix=van, useprefix=true},
  abstract = {The gaming industry has started to look for solutions in the Artificial intelligence (AI) research community and work has begun with common standards for integration. At the same time, few robotic systems in development use already developed AI frameworks and technologies. In this article, we present the development and evaluation of the Hazard framework that has been used to rapidly create simulations for development of cognitive systems. Implementations include for example a dialogue system that transparently can connect to either an Unmanned Aerial Vehicle (UAV) or a simulated counterpart. Hazard is found suitable for developing simulations supporting high-level AI development and we identify and propose a solution to the factors that make the framework unsuitable for lower level robotic specific tasks such as event/chronicle recognition.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\IM7V9B95\Aha et al. - Proceedings of the 2005 IJCAI Workshop on Reasoning, Representation, and Learning in Computer Games.pdf}
}

@video{alanlaboratoryAILearnsDodge2025,
  entrysubtype = {video},
  title = {{{AI Learns}} to {{Dodge Wrecking Balls}} (Deep Reinforcement Learning)},
  editor = {{AlanLaboratory}},
  editortype = {director},
  date = {2025-04-30},
  url = {https://www.youtube.com/watch?v=MCdDwZOSfYg},
  urldate = {2025-12-17},
  abstract = {In this video, an AI agent named Alan attempts to escape a dangerous wrecking zone. Alan faces three levels of increasing difficulty, from simple obstacles to complex and dynamic environments.}
}

@software{AlanLaboratoryUnrealMLAgents2025,
  title = {{{AlanLaboratory}}/{{UnrealMLAgents}}},
  date = {2025-12-05T21:27:42Z},
  origdate = {2024-06-08T19:37:29Z},
  url = {https://github.com/AlanLaboratory/UnrealMLAgents},
  urldate = {2025-12-17},
  abstract = {The Unreal ML Agents Toolkit is an open-source project that enables Unreal Engine games and simulations to serve as environments for training intelligent agents using deep reinforcement learning. This project is a port of Unity ML-Agents, adapted to work within Unreal Engine.},
  organization = {AlanLaboratory},
  keywords = {artificial-intelligence,deep-learning,deep-reinforcement-learning,machine-learning,neural-network,reinforcement-learning,unreal-engine,unreal-engine-5,unreal-engine-plugin}
}

@online{asc686f61MLFlowReinforcementLearning2025,
  title = {{{MLFlow}} for {{Reinforcement}} Learning},
  author = {Asc686f61},
  date = {2025-05-09T07:08:37},
  url = {https://medium.com/@asc686f61/mlflow-for-reinforcement-learning-1db8bc8bf0b4},
  urldate = {2025-12-15},
  abstract = {1. Introduction},
  langid = {english},
  organization = {Medium},
  file = {C:\Users\rini\Zotero\storage\3NZMISR9\mlflow-for-reinforcement-learning-1db8bc8bf0b4.html}
}

@article{boothAISystemsLeft,
  title = {The {{AI Systems}}  of  {{Left}} 4 {{Dead}}},
  author = {Booth, Michael},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\MLB6IS89\Booth - The AI Systems  of  Left 4 Dead.pdf}
}

@incollection{busoniuMultiagentReinforcementLearning2010,
  title = {Multi-Agent {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Multi-Agent {{Reinforcement Learning}}},
  booktitle = {Innovations in {{Multi-Agent Systems}} and {{Applications}} - 1},
  author = {Bu≈üoniu, Lucian and Babu≈°ka, Robert and De Schutter, Bart},
  editor = {Srinivasan, Dipti and Jain, Lakhmi C.},
  editora = {Kacprzyk, Janusz},
  editoratype = {redactor},
  date = {2010},
  volume = {310},
  pages = {183--221},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14435-6_7},
  url = {http://link.springer.com/10.1007/978-3-642-14435-6_7},
  urldate = {2025-12-15},
  abstract = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of Multi-Agent Reinforcement Learning (MARL) algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of MARL are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where MARL techniques have been applied are briefly discussed. Several MARL algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the MARL field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
  isbn = {978-3-642-14434-9 978-3-642-14435-6},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\GLL69R5S\Bu≈üoniu et al. - 2010 - Multi-agent Reinforcement Learning An Overview.pdf}
}

@inreference{CurriculumLearning2025,
  title = {Curriculum Learning},
  booktitle = {Wikipedia},
  date = {2025-07-17T19:53:19Z},
  url = {https://en.wikipedia.org/w/index.php?title=Curriculum_learning&oldid=1301041453},
  urldate = {2025-12-15},
  abstract = {Curriculum learning is a technique in machine learning in which a model is trained on examples of increasing difficulty, where the definition of "difficulty" may be provided externally or discovered as part of the training process. This is intended to attain good performance more quickly, or to converge to a better local optimum if the global optimum is not found.},
  langid = {english},
  annotation = {Page Version ID: 1301041453},
  file = {C:\Users\rini\Zotero\storage\E32AV7JR\Curriculum_learning.html}
}

@inproceedings{ferdousAgentBasedTesting3D2022,
  title = {Towards {{Agent-Based Testing}} of {{3D Games}} Using {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 37th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Ferdous, Raihana and Kifetew, Fitsum and Prandi, Davide and Susi, Angelo},
  date = {2022-10-10},
  pages = {1--8},
  publisher = {ACM},
  location = {Rochester MI USA},
  doi = {10.1145/3551349.3560507},
  url = {https://dl.acm.org/doi/10.1145/3551349.3560507},
  urldate = {2025-12-17},
  abstract = {Computer game is a billion-dollar industry and is booming. Testing games has been recognized as a difficult task, which mainly relies on manual playing and scripting based testing. With the advances in technologies, computer games have become increasingly more interactive and complex, thus play-testing using human participants alone has become unfeasible. In recent days, play-testing of games via autonomous agents has shown great promise by accelerating and simplifying this process. Reinforcement Learning solutions have the potential of complementing current scripted and automated solutions by learning directly from playing the game without the need of human intervention. This paper presented an approach based on reinforcement learning for automated testing of 3D games. We make use of the notion of curiosity as a motivating factor to encourage an RL agent to explore its environment. The results from our exploratory study are promising and we have preliminary evidence that reinforcement learning can be adopted for automated testing of 3D games.},
  eventtitle = {{{ASE}} '22: 37th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  isbn = {978-1-4503-9475-8},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\RFCHMA4S\Ferdous et al. - 2022 - Towards Agent-Based Testing of 3D Games using Reinforcement Learning.pdf}
}

@online{fuAReaLLargeScaleAsynchronous2025,
  title = {{{AReaL}}: {{A Large-Scale Asynchronous Reinforcement Learning System}} for {{Language Reasoning}}},
  shorttitle = {{{AReaL}}},
  author = {Fu, Wei and Gao, Jiaxuan and Shen, Xujie and Zhu, Chen and Mei, Zhiyu and He, Chuyi and Xu, Shusheng and Wei, Guo and Mei, Jun and Wang, Jiashu and Yang, Tongkai and Yuan, Binhang and Wu, Yi},
  date = {2025-11-25},
  eprint = {2505.24298},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.24298},
  url = {http://arxiv.org/abs/2505.24298},
  urldate = {2025-12-15},
  abstract = {Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77√ó training speedup compared to synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AREAL is available at https://github.com/inclusionAI/AReaL/.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\YIGCBSKP\Fu et al. - 2025 - AReaL A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning.pdf}
}

@video{gamemakerstoolkitWhatCapcomDidnt2015,
  entrysubtype = {video},
  title = {What {{Capcom Didn}}'t {{Tell You About Resident Evil}} 4},
  editor = {{Game Maker's Toolkit}},
  editortype = {director},
  date = {2015-06-02},
  url = {https://www.youtube.com/watch?v=zFv6KAdQ5SE},
  urldate = {2025-12-15},
  abstract = {üî¥ Get bonus content by supporting Game Maker‚Äôs Toolkit - https://gamemakerstoolkit.com/support/ üî¥ Resident Evil 4 did something really clever with its difficulty level. But what's even smarter is that developer Capcom never told gamers what it was doing. Find out more, in this episod}
}

@online{gordilloImprovingPlaytestingCoverage2021,
  title = {Improving {{Playtesting Coverage}} via {{Curiosity Driven Reinforcement Learning Agents}}},
  author = {Gordillo, Camilo and Bergdahl, Joakim and Tollmar, Konrad and Gissl√©n, Linus},
  date = {2021-06-23},
  eprint = {2103.13798},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.13798},
  url = {http://arxiv.org/abs/2103.13798},
  urldate = {2025-12-17},
  abstract = {As modern games continue growing both in size and complexity, it has become more challenging to ensure that all the relevant content is tested and that any potential issue is properly identified and fixed. Attempting to maximize testing coverage using only human participants, however, results in a tedious and hard to orchestrate process which normally slows down the development cycle. Complementing playtesting via autonomous agents has shown great promise accelerating and simplifying this process. This paper addresses the problem of automatically exploring and testing a given scenario using reinforcement learning agents trained to maximize game state coverage. Each of these agents is rewarded based on the novelty of its actions, thus encouraging a curious and exploratory behaviour on a complex 3D scenario where previously proposed exploration techniques perform poorly. The curious agents are able to learn the complex navigation mechanics required to reach the different areas around the map, thus providing the necessary data to identify potential issues. Moreover, the paper also explores different visualization strategies and evaluates how to make better use of the collected data to drive design decisions and to recognize possible problems and oversights.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\RPC9SL2W\Gordillo et al. - 2021 - Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents.pdf}
}

@online{GymGame,
  title = {Gym {{Game}}},
  url = {https://atari.neoinstinct.com/},
  urldate = {2025-12-15},
  file = {C:\Users\rini\Zotero\storage\DNRJ3SQ4\atari.neoinstinct.com.html}
}

@online{Home,
  title = {Home},
  url = {https://modl.ai/},
  urldate = {2025-12-15},
  abstract = {modl.ai uses AI agents and analysts to automate game testing and QA. Find bugs, glitches, and performance issues faster. No SDKs or code needed to get started.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\XWDXHPIE\modl.ai.html}
}

@online{ImprovingPlaytestingCoverage,
  title = {Improving {{Playtesting Coverage}} via {{Curiosity Driven Reinforcement Learning Agents}}},
  url = {https://video.itu.dk/video/71684715/improving-playtesting-coverage-via},
  urldate = {2025-12-15},
  abstract = {Author: Camilo Gordillo, Joakim Bergdahl, Konrad Tollmar and Linus Gisslen.Title: Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning...},
  organization = {IT University of Copenhagen},
  file = {C:\Users\rini\Zotero\storage\KAS84YCS\improving-playtesting-coverage-via.html}
}

@online{JulianTogelius,
  title = {Julian {{Togelius}}},
  url = {http://julian.togelius.com/},
  urldate = {2025-12-15},
  file = {C:\Users\rini\Zotero\storage\42C8F8HL\julian.togelius.com.html}
}

@article{kozmaTacklingGenerationCombat,
  title = {Tackling {{Generation}} of {{Combat Encounters}} in {{Role-playing Digital Games}}},
  author = {Kozma, Matou≈°},
  abstract = {Procedural content generation (PCG) has been used in digital games since the early 1980s. Here we focus on a new problem of generating personalized combat encounters in role playing video games (RPG). A game should provide a player with combat encounters of adequate difficulties, which ideally should be matching the player‚Äôs performance in order for a game to provide adequate challenge to the player. In this paper, we describe our own reinforcement learning algorithm that estimates difficulties of combat encounters during game runtime, which can be them used to find next suitable combat encounter of desired difficulty in a stochastic hill-climbing manner. After a player finishes the encounter, its result is propagated through the matrix to update the estimations of not only the presented combat encounter, but also similar ones. To test our solution, we conducted a preliminary study with human players on a simplified RPG game we have developed. The data collected suggests our algorithm can adapt the matrix to the player performance fast from little amounts of data, even though not precisely.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\PRCFUY3X\Kozma - Tackling Generation of Combat Encounters in Role-playing Digital Games.pdf}
}

@article{lepelletierdewoillemontAutomatedPlayTestingRL2022,
  title = {Automated {{Play-Testing}} through {{RL Based Human-Like Play-Styles Generation}}},
  author = {Le Pelletier De Woillemont, Pierre and Labory, R√©mi and Corruble, Vincent},
  date = {2022-10-11},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  shortjournal = {AIIDE},
  volume = {18},
  number = {1},
  pages = {146--154},
  issn = {2334-0924, 2326-909X},
  doi = {10.1609/aiide.v18i1.21958},
  url = {https://ojs.aaai.org/index.php/AIIDE/article/view/21958},
  urldate = {2025-12-15},
  abstract = {The increasing complexity of gameplay mechanisms in modern video games is leading to the emergence of a wider range of ways to play games. The variety of possible play-styles needs to be anticipated by designers, through automated tests. Reinforcement Learning is a promising answer to the need of automating video game testing. To that effect one needs to train an agent to play the game, while ensuring this agent will generate the same play-styles as the players in order to give meaningful feedback to the designers. We present CARMI: a Configurable Agent with Relative Metrics as Input. An agent able to emulate the players play-styles, even on previously unseen levels. Unlike current methods it does not rely on having full trajectories, but only summary data. Moreover it only requires little human data, thus compatible with the constraints of modern video game production. This novel agent could be used to investigate behaviors and balancing during the production of a video game with a realistic amount of training time.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\JL228QVX\Le Pelletier De Woillemont et al. - 2022 - Automated Play-Testing through RL Based Human-Like Play-Styles Generation.pdf}
}

@online{MachineLearningAi,
  title = {Machine Learning Ai Tutorial for {{UE5}} - {{YouTube}}},
  url = {https://www.youtube.com/},
  urldate = {2025-12-17},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\U838DZ9Y\www.youtube.com.html}
}

@video{mariocReinforcementLearningUnreal2022,
  entrysubtype = {video},
  title = {Reinforcement {{Learning}} with {{Unreal Engine}} 5 and {{OpenAI Gym}} (Ur10)},
  editor = {{Mario C}},
  editortype = {director},
  date = {2022-07-17},
  url = {https://www.youtube.com/watch?v=2IV5gqnlgZw},
  urldate = {2025-12-17},
  abstract = {Hi everyone :), Here I am again with a new tech demo of what is achievable with ‚Ä™@UnrealEngine‚Ä¨  and Machine Learning.}
}

@video{neuralbreakdownwithavbHowMultiAgentAI2023,
  entrysubtype = {video},
  title = {How {{Multi-Agent AI}} Learn by Continuously Competing against Themselves | {{Self Play}}},
  editor = {{Neural Breakdown with AVB}},
  editortype = {director},
  date = {2023-09-27},
  url = {https://www.youtube.com/watch?v=0WMvzpAYJGk},
  urldate = {2025-12-15},
  abstract = {In this video, I discuss the intuition and brilliance behind Self Play - a standard reinforcement learning algorithm that has trained many multi-agent RL AI like Alpha Go Zero, Leela Chess Zero, the Dota 2 AI, and multiple simulation projects like "You shall not pass", "Kick and Defend", "Sumo Wrestle", and the team-based "Hide and Seek" by OpenAI.}
}

@inproceedings{noblegaAdaptiveDeepReinforcement2025,
  title = {Towards {{Adaptive Deep Reinforcement Game Balancing}}},
  author = {Noblega, Ashey and Paes, Aline and Clua, Esteban},
  date = {2025-12-17},
  pages = {693--700},
  url = {https://www.scitepress.org/Link.aspx?doi=10.5220/0007395406930700},
  urldate = {2025-12-17},
  abstract = {Digital Library},
  eventtitle = {11th {{International Conference}} on {{Agents}} and {{Artificial Intelligence}}},
  isbn = {978-989-758-350-6}
}

@video{openaiMultiAgentHideSeek2019,
  entrysubtype = {video},
  title = {Multi-{{Agent Hide}} and {{Seek}}},
  editor = {{OpenAI}},
  editortype = {director},
  date = {2019-09-17},
  url = {https://www.youtube.com/watch?v=kopoLzvh5jY},
  urldate = {2025-12-17},
  abstract = {We‚Äôve observed agents discovering progressively more complex tool use while playing a simple game of hide-and-seek. Through training in our new simulated hide-and-seek environment, agents build a series of six distinct strategies and counterstrategies, some of which we did not know our environment supported. The self-supervised emergent complexity in this simple environment further suggests that multi-agent co-adaptation may one day produce extremely complex and intelligent behavior.}
}

@video{orobixReinforcementLearningAutomating2023,
  entrysubtype = {video},
  title = {Reinforcement Learning for Automating Game Testing - {{Open-world}} Games},
  editor = {{Or√≤bix}},
  editortype = {director},
  date = {2023-10-04},
  url = {https://www.youtube.com/watch?v=7XEBT2msUUQ},
  urldate = {2025-12-15},
  abstract = {In open-world games, ensuring they are playable and bug-free is crucial, but is becoming increasingly difficult and time-consuming using manual game testing. Maximizing ex}
}

@online{papoudakisDealingNonStationarityMultiAgent2019,
  title = {Dealing with {{Non-Stationarity}} in {{Multi-Agent Deep Reinforcement Learning}}},
  author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V.},
  date = {2019-06-11},
  eprint = {1906.04737},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.04737},
  url = {http://arxiv.org/abs/1906.04737},
  urldate = {2025-12-15},
  abstract = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent‚Äôs policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\I886ME9U\Papoudakis et al. - 2019 - Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning.pdf}
}

@article{ProposalReinforcementLearning,
  title = {Proposal: {{Reinforcement Learning Agents}} for {{Automated Playtesting}} and {{Game Balancing}} in *{{Project Paj}}},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\TUJGQFNT\Proposal Reinforcement Learning Agents for Automated Playtesting and Game Balancing in Project Paj.pdf}
}

@online{ReinforcementLearning,
  title = {Reinforcement {{Learning}}},
  url = {http://www.youtube.com/playlist?list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM},
  urldate = {2025-12-15},
  abstract = {This series provides an overview of reinforcement learning, a type of machine learning that has the potential to solve some control system problems that are ...},
  langid = {english},
  organization = {YouTube},
  file = {C:\Users\rini\Zotero\storage\Y3T3IMZP\playlist.html}
}

@online{remote_marzipan_749RLNotHeavily2025,
  type = {Reddit Post},
  title = {{{RL}} Not Heavily Used for Game Testing?},
  author = {Remote\_Marzipan\_749},
  date = {2025-08-07T21:18:01},
  url = {https://www.reddit.com/r/reinforcementlearning/comments/1mkcdu8/rl_not_heavily_used_for_game_testing/},
  urldate = {2025-12-15},
  organization = {r/reinforcementlearning},
  file = {C:\Users\rini\Zotero\storage\JUM2WNRE\rl_not_heavily_used_for_game_testing.html}
}

@online{RewardShapingMastering,
  title = {Reward Shaping ‚Äî {{Mastering Reinforcement Learning}}},
  url = {https://gibberblot.github.io/rl-notes/single-agent/reward-shaping.html},
  urldate = {2025-12-15}
}

@article{ruppSimulationDrivenBalancingCompetitive2024,
  title = {Simulation-{{Driven Balancing}} of {{Competitive Game Levels}} with {{Reinforcement Learning}}},
  author = {Rupp, Florian and Eberhardinger, Manuel and Eckert, Kai},
  date = {2024-12},
  journaltitle = {IEEE Transactions on Games},
  shortjournal = {IEEE Trans. Games},
  volume = {16},
  number = {4},
  eprint = {2503.18748},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {903--913},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2024.3399536},
  url = {http://arxiv.org/abs/2503.18748},
  urldate = {2025-12-15},
  abstract = {The balancing process for game levels in competitive two-player contexts involves a lot of manual work and testing, particularly for non-symmetrical game levels. In this work, we frame game balancing as a procedural content generation task and propose an architecture for automatically balancing of tile-based levels within the PCGRL framework (procedural content generation via reinforcement learning). Our architecture is divided into three parts: (1) a level generator, (2) a balancing agent, and (3) a reward modeling simulation. Through repeated simulations, the balancing agent receives rewards for adjusting the level towards a given balancing objective, such as equal win rates for all players. To this end, we propose new swapbased representations to improve the robustness of playability, thereby enabling agents to balance game levels more effectively and quickly compared to traditional PCGRL. By analyzing the agent‚Äôs swapping behavior, we can infer which tile types have the most impact on the balance. We validate our approach in the Neural MMO (NMMO) environment in a competitive two-player scenario. In this extended conference paper, we present improved results, explore the applicability of the method to various forms of balancing beyond equal balancing, compare the performance to another search-based approach, and discuss the application of existing fairness metrics to game balancing.},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Preprint of the journal (IEEE Transactions on Games) paper of the same name},
  file = {C:\Users\rini\Zotero\storage\5A4PMYDE\Rupp et al. - 2024 - Simulation-Driven Balancing of Competitive Game Levels with Reinforcement Learning.pdf}
}

@online{SimulationDrivenBalancingCompetitive,
  title = {Simulation-{{Driven Balancing}} of {{Competitive Game Levels}} with {{Reinforcement Learning This}} Research Was Supported by the {{Volkswagen Foundation}} ({{Project}}: {{Consequences}} of {{Artificial Intelligence}} on {{Urban Societies}}, {{Grant}} 98555).},
  url = {https://arxiv.org/html/2503.18748v1},
  urldate = {2025-12-15}
}

@online{specificpark2594AutomaticGameBalancing2023,
  type = {Reddit Post},
  title = {Automatic Game Balancing with {{Reinforcement Learning}}},
  author = {SpecificPark2594},
  date = {2023-02-13T20:52:50},
  url = {https://www.reddit.com/r/gamedesign/comments/111jnyt/automatic_game_balancing_with_reinforcement/},
  urldate = {2025-12-15},
  organization = {r/gamedesign},
  file = {C:\Users\rini\Zotero\storage\5HZHCCMJ\automatic_game_balancing_with_reinforcement.html}
}

@incollection{tanMultiAgentReinforcementLearning1993,
  title = {Multi-{{Agent Reinforcement Learning}}: {{Independent}} vs. {{Cooperative Agents}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1993},
  author = {Tan, Ming},
  date = {1993},
  pages = {330--337},
  publisher = {Elsevier},
  doi = {10.1016/B978-1-55860-307-3.50049-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603073500496},
  urldate = {2025-12-15},
  isbn = {978-1-55860-307-3},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\6UUJM943\Tan - 1993 - Multi-Agent Reinforcement Learning Independent vs. Cooperative Agents.pdf}
}

@article{yannakakisRealTimeGameAdaptation2009,
  title = {Real-{{Time Game Adaptation}} for {{Optimizing Player Satisfaction}}},
  author = {Yannakakis, G.N. and Hallam, J.},
  date = {2009-06},
  journaltitle = {IEEE Transactions on Computational Intelligence and AI in Games},
  shortjournal = {IEEE Trans. Comput. Intell. AI Games},
  volume = {1},
  number = {2},
  pages = {121--133},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2009.2024533},
  url = {http://ieeexplore.ieee.org/document/5067382/},
  urldate = {2025-12-15},
  abstract = {A methodology for optimizing player satisfaction in games on the ‚ÄúPlayware‚Äù physical interactive platform is demonstrated in this paper. Previously constructed artificial neural network user models, reported in the literature, map individual playing characteristics to reported entertainment preferences for augmented-reality game players. An adaptive mechanism then adjusts controllable game parameters in real time in order to improve the entertainment value of the game for the player. The basic approach presented here applies gradient ascent to the user model to suggest the direction of parameter adjustment that leads toward games of higher entertainment value. A simple rule set exploits the derivative information to adjust specific game parameters to augment the entertainment value. Those adjustments take place frequently during the game with interadjustment intervals that maintain the user model‚Äôs accuracy. Performance of the adaptation mechanism is evaluated using a game survey experiment. Results indicate the efficacy and robustness of the mechanism in adapting the game according to a user‚Äôs individual playing features and enhancing the gameplay experience. The limitations and the use of the methodology as an effective adaptive mechanism for entertainment capture and augmentation are discussed.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\MWW3XPYD\Yannakakis and Hallam - 2009 - Real-Time Game Adaptation for Optimizing Player Satisfaction.pdf}
}
