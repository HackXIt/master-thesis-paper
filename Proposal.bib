@article{ahaProceedings2005IJCAI,
  title = {Proceedings of the 2005 {{IJCAI Workshop}} on {{Reasoning}}, {{Representation}}, and {{Learning}} in {{Computer Games}}},
  author = {Aha, David W and Mu√±oz-Avila, H√©ctor and family=Lent, given=Michael, prefix=van, useprefix=true},
  abstract = {The gaming industry has started to look for solutions in the Artificial intelligence (AI) research community and work has begun with common standards for integration. At the same time, few robotic systems in development use already developed AI frameworks and technologies. In this article, we present the development and evaluation of the Hazard framework that has been used to rapidly create simulations for development of cognitive systems. Implementations include for example a dialogue system that transparently can connect to either an Unmanned Aerial Vehicle (UAV) or a simulated counterpart. Hazard is found suitable for developing simulations supporting high-level AI development and we identify and propose a solution to the factors that make the framework unsuitable for lower level robotic specific tasks such as event/chronicle recognition.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\IM7V9B95\Aha et al. - Proceedings of the 2005 IJCAI Workshop on Reasoning, Representation, and Learning in Computer Games.pdf}
}

@video{alanlaboratoryAILearnsDodge2025,
  entrysubtype = {video},
  title = {{{AI Learns}} to {{Dodge Wrecking Balls}} (Deep Reinforcement Learning)},
  editor = {{AlanLaboratory}},
  editortype = {director},
  date = {2025-04-30},
  url = {https://www.youtube.com/watch?v=MCdDwZOSfYg},
  urldate = {2025-12-17},
  abstract = {In this video, an AI agent named Alan attempts to escape a dangerous wrecking zone. Alan faces three levels of increasing difficulty, from simple obstacles to complex and dynamic environments.}
}

@software{AlanLaboratoryUnrealMLAgents2025,
  title = {{{AlanLaboratory}}/{{UnrealMLAgents}}},
  date = {2025-12-05T21:27:42Z},
  origdate = {2024-06-08T19:37:29Z},
  url = {https://github.com/AlanLaboratory/UnrealMLAgents},
  urldate = {2025-12-17},
  abstract = {The Unreal ML Agents Toolkit is an open-source project that enables Unreal Engine games and simulations to serve as environments for training intelligent agents using deep reinforcement learning. This project is a port of Unity ML-Agents, adapted to work within Unreal Engine.},
  organization = {AlanLaboratory},
  keywords = {artificial-intelligence,deep-learning,deep-reinforcement-learning,machine-learning,neural-network,reinforcement-learning,unreal-engine,unreal-engine-5,unreal-engine-plugin}
}

@online{asc686f61MLFlowReinforcementLearning2025,
  title = {{{MLFlow}} for {{Reinforcement}} Learning},
  author = {Asc686f61},
  date = {2025-05-09T07:08:37},
  url = {https://medium.com/@asc686f61/mlflow-for-reinforcement-learning-1db8bc8bf0b4},
  urldate = {2025-12-15},
  abstract = {1. Introduction},
  langid = {english},
  organization = {Medium},
  file = {C:\Users\rini\Zotero\storage\3NZMISR9\mlflow-for-reinforcement-learning-1db8bc8bf0b4.html}
}

@inproceedings{bengioCurriculumLearning2009,
  title = {Curriculum Learning},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Bengio, Yoshua and Louradour, J√©r√¥me and Collobert, Ronan and Weston, Jason},
  date = {2009-06-14},
  pages = {41--48},
  publisher = {ACM},
  location = {Montreal Quebec Canada},
  doi = {10.1145/1553374.1553380},
  url = {https://dl.acm.org/doi/10.1145/1553374.1553380},
  urldate = {2026-01-29},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ‚Äúcurriculum learning‚Äù. In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  eventtitle = {{{ICML}} '09: {{The}} 26th {{Annual International Conference}} on {{Machine Learning}} Held in Conjunction with the 2007 {{International Conference}} on {{Inductive Logic Programming}}},
  isbn = {978-1-60558-516-1},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\6ZPQGX8B\Bengio et al. - 2009 - Curriculum learning.pdf}
}

@article{boothAISystemsLeft,
  title = {The {{AI Systems}}  of  {{Left}} 4 {{Dead}}},
  author = {Booth, Michael},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\MLB6IS89\Booth - The AI Systems  of  Left 4 Dead.pdf}
}

@incollection{busoniuMultiagentReinforcementLearning2010,
  title = {Multi-Agent {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Multi-Agent {{Reinforcement Learning}}},
  booktitle = {Innovations in {{Multi-Agent Systems}} and {{Applications}} - 1},
  author = {Bu≈üoniu, Lucian and Babu≈°ka, Robert and De Schutter, Bart},
  editor = {Srinivasan, Dipti and Jain, Lakhmi C.},
  editora = {Kacprzyk, Janusz},
  editoratype = {redactor},
  date = {2010},
  volume = {310},
  pages = {183--221},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14435-6_7},
  url = {http://link.springer.com/10.1007/978-3-642-14435-6_7},
  urldate = {2025-12-15},
  abstract = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of Multi-Agent Reinforcement Learning (MARL) algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of MARL are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where MARL techniques have been applied are briefly discussed. Several MARL algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the MARL field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
  isbn = {978-3-642-14434-9 978-3-642-14435-6},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\GLL69R5S\Bu≈üoniu et al. - 2010 - Multi-agent Reinforcement Learning An Overview.pdf}
}

@inreference{CurriculumLearning2025,
  title = {Curriculum Learning},
  booktitle = {Wikipedia},
  date = {2025-07-17T19:53:19Z},
  url = {https://en.wikipedia.org/w/index.php?title=Curriculum_learning&oldid=1301041453},
  urldate = {2025-12-15},
  abstract = {Curriculum learning is a technique in machine learning in which a model is trained on examples of increasing difficulty, where the definition of "difficulty" may be provided externally or discovered as part of the training process. This is intended to attain good performance more quickly, or to converge to a better local optimum if the global optimum is not found.},
  langid = {english},
  annotation = {Page Version ID: 1301041453},
  file = {C:\Users\rini\Zotero\storage\E32AV7JR\Curriculum_learning.html}
}

@inproceedings{ferdousAgentBasedTesting3D2022,
  title = {Towards {{Agent-Based Testing}} of {{3D Games}} Using {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 37th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Ferdous, Raihana and Kifetew, Fitsum and Prandi, Davide and Susi, Angelo},
  date = {2022-10-10},
  pages = {1--8},
  publisher = {ACM},
  location = {Rochester MI USA},
  doi = {10.1145/3551349.3560507},
  url = {https://dl.acm.org/doi/10.1145/3551349.3560507},
  urldate = {2025-12-17},
  abstract = {Computer game is a billion-dollar industry and is booming. Testing games has been recognized as a difficult task, which mainly relies on manual playing and scripting based testing. With the advances in technologies, computer games have become increasingly more interactive and complex, thus play-testing using human participants alone has become unfeasible. In recent days, play-testing of games via autonomous agents has shown great promise by accelerating and simplifying this process. Reinforcement Learning solutions have the potential of complementing current scripted and automated solutions by learning directly from playing the game without the need of human intervention. This paper presented an approach based on reinforcement learning for automated testing of 3D games. We make use of the notion of curiosity as a motivating factor to encourage an RL agent to explore its environment. The results from our exploratory study are promising and we have preliminary evidence that reinforcement learning can be adopted for automated testing of 3D games.},
  eventtitle = {{{ASE}} '22: 37th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  isbn = {978-1-4503-9475-8},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\RFCHMA4S\Ferdous et al. - 2022 - Towards Agent-Based Testing of 3D Games using Reinforcement Learning.pdf}
}

@online{fuAReaLLargeScaleAsynchronous2025,
  title = {{{AReaL}}: {{A Large-Scale Asynchronous Reinforcement Learning System}} for {{Language Reasoning}}},
  shorttitle = {{{AReaL}}},
  author = {Fu, Wei and Gao, Jiaxuan and Shen, Xujie and Zhu, Chen and Mei, Zhiyu and He, Chuyi and Xu, Shusheng and Wei, Guo and Mei, Jun and Wang, Jiashu and Yang, Tongkai and Yuan, Binhang and Wu, Yi},
  date = {2025-11-25},
  eprint = {2505.24298},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.24298},
  url = {http://arxiv.org/abs/2505.24298},
  urldate = {2025-12-15},
  abstract = {Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77√ó training speedup compared to synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AREAL is available at https://github.com/inclusionAI/AReaL/.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\YIGCBSKP\Fu et al. - 2025 - AReaL A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning.pdf}
}

@video{gamemakerstoolkitWhatCapcomDidnt2015,
  entrysubtype = {video},
  title = {What {{Capcom Didn}}'t {{Tell You About Resident Evil}} 4},
  editor = {{Game Maker's Toolkit}},
  editortype = {director},
  date = {2015-06-02},
  url = {https://www.youtube.com/watch?v=zFv6KAdQ5SE},
  urldate = {2025-12-15},
  abstract = {üî¥ Get bonus content by supporting Game Maker‚Äôs Toolkit - https://gamemakerstoolkit.com/support/ üî¥ Resident Evil 4 did something really clever with its difficulty level. But what's even smarter is that developer Capcom never told gamers what it was doing. Find out more, in this episod}
}

@online{gordilloImprovingPlaytestingCoverage2021,
  title = {Improving {{Playtesting Coverage}} via {{Curiosity Driven Reinforcement Learning Agents}}},
  author = {Gordillo, Camilo and Bergdahl, Joakim and Tollmar, Konrad and Gissl√©n, Linus},
  date = {2021-06-23},
  eprint = {2103.13798},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.13798},
  url = {http://arxiv.org/abs/2103.13798},
  urldate = {2025-12-17},
  abstract = {As modern games continue growing both in size and complexity, it has become more challenging to ensure that all the relevant content is tested and that any potential issue is properly identified and fixed. Attempting to maximize testing coverage using only human participants, however, results in a tedious and hard to orchestrate process which normally slows down the development cycle. Complementing playtesting via autonomous agents has shown great promise accelerating and simplifying this process. This paper addresses the problem of automatically exploring and testing a given scenario using reinforcement learning agents trained to maximize game state coverage. Each of these agents is rewarded based on the novelty of its actions, thus encouraging a curious and exploratory behaviour on a complex 3D scenario where previously proposed exploration techniques perform poorly. The curious agents are able to learn the complex navigation mechanics required to reach the different areas around the map, thus providing the necessary data to identify potential issues. Moreover, the paper also explores different visualization strategies and evaluates how to make better use of the collected data to drive design decisions and to recognize possible problems and oversights.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\RPC9SL2W\Gordillo et al. - 2021 - Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents.pdf}
}

@online{GymGame,
  title = {Gym {{Game}}},
  url = {https://atari.neoinstinct.com/},
  urldate = {2025-12-15},
  file = {C:\Users\rini\Zotero\storage\DNRJ3SQ4\atari.neoinstinct.com.html}
}

@online{Home,
  title = {Home},
  url = {https://modl.ai/},
  urldate = {2025-12-15},
  abstract = {modl.ai uses AI agents and analysts to automate game testing and QA. Find bugs, glitches, and performance issues faster. No SDKs or code needed to get started.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\XWDXHPIE\modl.ai.html}
}

@online{ImprovingPlaytestingCoverage,
  title = {Improving {{Playtesting Coverage}} via {{Curiosity Driven Reinforcement Learning Agents}}},
  url = {https://video.itu.dk/video/71684715/improving-playtesting-coverage-via},
  urldate = {2025-12-15},
  abstract = {Author: Camilo Gordillo, Joakim Bergdahl, Konrad Tollmar and Linus Gisslen.Title: Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning...},
  organization = {IT University of Copenhagen},
  file = {C:\Users\rini\Zotero\storage\KAS84YCS\improving-playtesting-coverage-via.html}
}

@online{JulianTogelius,
  title = {Julian {{Togelius}}},
  url = {http://julian.togelius.com/},
  urldate = {2025-12-15},
  file = {C:\Users\rini\Zotero\storage\42C8F8HL\julian.togelius.com.html}
}

@article{kozmaTacklingGenerationCombat,
  title = {Tackling {{Generation}} of {{Combat Encounters}} in {{Role-playing Digital Games}}},
  author = {Kozma, Matou≈°},
  abstract = {Procedural content generation (PCG) has been used in digital games since the early 1980s. Here we focus on a new problem of generating personalized combat encounters in role playing video games (RPG). A game should provide a player with combat encounters of adequate difficulties, which ideally should be matching the player‚Äôs performance in order for a game to provide adequate challenge to the player. In this paper, we describe our own reinforcement learning algorithm that estimates difficulties of combat encounters during game runtime, which can be them used to find next suitable combat encounter of desired difficulty in a stochastic hill-climbing manner. After a player finishes the encounter, its result is propagated through the matrix to update the estimations of not only the presented combat encounter, but also similar ones. To test our solution, we conducted a preliminary study with human players on a simplified RPG game we have developed. The data collected suggests our algorithm can adapt the matrix to the player performance fast from little amounts of data, even though not precisely.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\PRCFUY3X\Kozma - Tackling Generation of Combat Encounters in Role-playing Digital Games.pdf}
}

@article{lepelletierdewoillemontAutomatedPlayTestingRL2022,
  title = {Automated {{Play-Testing}} through {{RL Based Human-Like Play-Styles Generation}}},
  author = {Le Pelletier De Woillemont, Pierre and Labory, R√©mi and Corruble, Vincent},
  date = {2022-10-11},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  shortjournal = {AIIDE},
  volume = {18},
  number = {1},
  pages = {146--154},
  issn = {2334-0924, 2326-909X},
  doi = {10.1609/aiide.v18i1.21958},
  url = {https://ojs.aaai.org/index.php/AIIDE/article/view/21958},
  urldate = {2025-12-15},
  abstract = {The increasing complexity of gameplay mechanisms in modern video games is leading to the emergence of a wider range of ways to play games. The variety of possible play-styles needs to be anticipated by designers, through automated tests. Reinforcement Learning is a promising answer to the need of automating video game testing. To that effect one needs to train an agent to play the game, while ensuring this agent will generate the same play-styles as the players in order to give meaningful feedback to the designers. We present CARMI: a Configurable Agent with Relative Metrics as Input. An agent able to emulate the players play-styles, even on previously unseen levels. Unlike current methods it does not rely on having full trajectories, but only summary data. Moreover it only requires little human data, thus compatible with the constraints of modern video game production. This novel agent could be used to investigate behaviors and balancing during the production of a video game with a realistic amount of training time.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\JL228QVX\Le Pelletier De Woillemont et al. - 2022 - Automated Play-Testing through RL Based Human-Like Play-Styles Generation.pdf}
}

@online{MachineLearningAi,
  title = {Machine Learning Ai Tutorial for {{UE5}} - {{YouTube}}},
  url = {https://www.youtube.com/},
  urldate = {2025-12-17},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\U838DZ9Y\www.youtube.com.html}
}

@video{mariocReinforcementLearningUnreal2022,
  entrysubtype = {video},
  title = {Reinforcement {{Learning}} with {{Unreal Engine}} 5 and {{OpenAI Gym}} (Ur10)},
  editor = {{Mario C}},
  editortype = {director},
  date = {2022-07-17},
  url = {https://www.youtube.com/watch?v=2IV5gqnlgZw},
  urldate = {2025-12-17},
  abstract = {Hi everyone :), Here I am again with a new tech demo of what is achievable with ‚Ä™@UnrealEngine‚Ä¨  and Machine Learning.}
}

@video{neuralbreakdownwithavbHowMultiAgentAI2023,
  entrysubtype = {video},
  title = {How {{Multi-Agent AI}} Learn by Continuously Competing against Themselves | {{Self Play}}},
  editor = {{Neural Breakdown with AVB}},
  editortype = {director},
  date = {2023-09-27},
  url = {https://www.youtube.com/watch?v=0WMvzpAYJGk},
  urldate = {2025-12-15},
  abstract = {In this video, I discuss the intuition and brilliance behind Self Play - a standard reinforcement learning algorithm that has trained many multi-agent RL AI like Alpha Go Zero, Leela Chess Zero, the Dota 2 AI, and multiple simulation projects like "You shall not pass", "Kick and Defend", "Sumo Wrestle", and the team-based "Hide and Seek" by OpenAI.}
}

@inproceedings{ngPolicyInvarianceReward1999,
  title = {Policy {{Invariance Under Reward Transformations}}: {{Theory}} and {{Application}} to {{Reward Shaping}}},
  shorttitle = {Policy {{Invariance Under Reward Transformations}}},
  author = {Ng, A. and Harada, Daishi and Russell, Stuart J.},
  date = {1999-06-27},
  url = {https://www.semanticscholar.org/paper/Policy-Invariance-Under-Reward-Transformations%3A-and-Ng-Harada/94066dc12fe31e96af7557838159bde598cb4f10},
  urldate = {2026-01-29},
  abstract = {This paper investigates conditions under which modi(cid:12)cations to the reward function of a Markov decision process preserve the optimal policy. It is shown that, besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the di(cid:11)erence in value of an arbitrary potential function applied to those states. Further-more, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known \textbackslash bugs" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  note = {[TLDR] Some well-known "bugs" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics that can lead to substantial reductions in learning time.},
  file = {C:\Users\rini\Zotero\storage\K7CVMJBC\Ng et al. - 1999 - Policy Invariance Under Reward Transformations Theory and Application to Reward Shaping.pdf}
}

@inproceedings{noblegaAdaptiveDeepReinforcement2019,
  title = {Towards {{Adaptive Deep Reinforcement Game Balancing}}:},
  shorttitle = {Towards {{Adaptive Deep Reinforcement Game Balancing}}},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Agents}} and {{Artificial Intelligence}}},
  author = {Noblega, Ashey and Paes, Aline and Clua, Esteban},
  date = {2019},
  pages = {693--700},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  location = {Prague, Czech Republic},
  doi = {10.5220/0007395406930700},
  url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0007395406930700},
  urldate = {2026-01-29},
  abstract = {The experience of a player regarding the difficulty of a video game is one of the main reasons for he/she decide to keep playing the game or abandon it. Effectively, player retention is one of the primary concerns related to the game development process. However, the experience of a player with a game is unique, making impractical to anticipate how they will face the gameplay. This work leverages the recent advances in Reinforcement Learning (RL) and Deep Learning (DL) to create intelligent agents that are able to adapt to the abilities of distinct players. We focus on balancing the difficulty of the game based on the information that the agent observes from the 3D environment as well as the current state of the game. In order to design an agent that learns how to act while still maintaining the balancing, we propose a reward function based on a balancing constant. We require that the agent remains inside a range around this constant during the training. Our experimental results show that by using such a reward function and combining information from different types of players it is possible to have adaptable agents that fit the player.},
  eventtitle = {11th {{International Conference}} on {{Agents}} and {{Artificial Intelligence}}},
  isbn = {978-989-758-350-6},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\CGLPBZTQ\Noblega et al. - 2019 - Towards Adaptive Deep Reinforcement Game Balancing.pdf}
}

@online{openaiDota2Large2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dƒôbiak, Przemys≈Çaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J√≥zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  date = {2019-12-13},
  eprint = {1912.06680},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1912.06680},
  url = {http://arxiv.org/abs/1912.06680},
  urldate = {2026-01-29},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\ESSQCPSE\OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learning.pdf}
}

@video{openaiMultiAgentHideSeek2019,
  entrysubtype = {video},
  title = {Multi-{{Agent Hide}} and {{Seek}}},
  editor = {{OpenAI}},
  editortype = {director},
  date = {2019-09-17},
  url = {https://www.youtube.com/watch?v=kopoLzvh5jY},
  urldate = {2025-12-17},
  abstract = {We‚Äôve observed agents discovering progressively more complex tool use while playing a simple game of hide-and-seek. Through training in our new simulated hide-and-seek environment, agents build a series of six distinct strategies and counterstrategies, some of which we did not know our environment supported. The self-supervised emergent complexity in this simple environment further suggests that multi-agent co-adaptation may one day produce extremely complex and intelligent behavior.}
}

@video{orobixReinforcementLearningAutomating2023,
  entrysubtype = {video},
  title = {Reinforcement Learning for Automating Game Testing - {{Open-world}} Games},
  editor = {{Or√≤bix}},
  editortype = {director},
  date = {2023-10-04},
  url = {https://www.youtube.com/watch?v=7XEBT2msUUQ},
  urldate = {2025-12-15},
  abstract = {In open-world games, ensuring they are playable and bug-free is crucial, but is becoming increasingly difficult and time-consuming using manual game testing. Maximizing ex}
}

@online{papoudakisDealingNonStationarityMultiAgent2019,
  title = {Dealing with {{Non-Stationarity}} in {{Multi-Agent Deep Reinforcement Learning}}},
  author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V.},
  date = {2019-06-11},
  eprint = {1906.04737},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.04737},
  url = {http://arxiv.org/abs/1906.04737},
  urldate = {2025-12-15},
  abstract = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent‚Äôs policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\I886ME9U\Papoudakis et al. - 2019 - Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning.pdf}
}

@article{ProposalReinforcementLearning,
  title = {Proposal: {{Reinforcement Learning Agents}} for {{Automated Playtesting}} and {{Game Balancing}} in *{{Project Paj}}},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\TUJGQFNT\Proposal Reinforcement Learning Agents for Automated Playtesting and Game Balancing in Project Paj.pdf}
}

@online{ReinforcementLearning,
  title = {Reinforcement {{Learning}}},
  url = {http://www.youtube.com/playlist?list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM},
  urldate = {2025-12-15},
  abstract = {This series provides an overview of reinforcement learning, a type of machine learning that has the potential to solve some control system problems that are ...},
  langid = {english},
  organization = {YouTube},
  file = {C:\Users\rini\Zotero\storage\Y3T3IMZP\playlist.html}
}

@online{remote_marzipan_749RLNotHeavily2025,
  type = {Reddit Post},
  title = {{{RL}} Not Heavily Used for Game Testing?},
  author = {Remote\_Marzipan\_749},
  date = {2025-08-07T21:18:01},
  url = {https://www.reddit.com/r/reinforcementlearning/comments/1mkcdu8/rl_not_heavily_used_for_game_testing/},
  urldate = {2025-12-15},
  organization = {r/reinforcementlearning},
  file = {C:\Users\rini\Zotero\storage\JUM2WNRE\rl_not_heavily_used_for_game_testing.html}
}

@online{RewardShapingMastering,
  title = {Reward Shaping ‚Äî {{Mastering Reinforcement Learning}}},
  url = {https://gibberblot.github.io/rl-notes/single-agent/reward-shaping.html},
  urldate = {2025-12-15}
}

@software{RstudioMlfloworiginal2023,
  title = {Rstudio/Mlflow-Original},
  date = {2023-01-28T20:03:18Z},
  origdate = {2018-07-24T17:29:40Z},
  url = {https://github.com/rstudio/mlflow-original},
  urldate = {2026-01-29},
  abstract = {Open source platform for the complete machine learning lifecycle},
  organization = {RStudio}
}

@article{ruppSimulationDrivenBalancingCompetitive2024,
  title = {Simulation-{{Driven Balancing}} of {{Competitive Game Levels}} with {{Reinforcement Learning}}},
  author = {Rupp, Florian and Eberhardinger, Manuel and Eckert, Kai},
  date = {2024-12},
  journaltitle = {IEEE Transactions on Games},
  shortjournal = {IEEE Trans. Games},
  volume = {16},
  number = {4},
  eprint = {2503.18748},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {903--913},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2024.3399536},
  url = {http://arxiv.org/abs/2503.18748},
  urldate = {2025-12-15},
  abstract = {The balancing process for game levels in competitive two-player contexts involves a lot of manual work and testing, particularly for non-symmetrical game levels. In this work, we frame game balancing as a procedural content generation task and propose an architecture for automatically balancing of tile-based levels within the PCGRL framework (procedural content generation via reinforcement learning). Our architecture is divided into three parts: (1) a level generator, (2) a balancing agent, and (3) a reward modeling simulation. Through repeated simulations, the balancing agent receives rewards for adjusting the level towards a given balancing objective, such as equal win rates for all players. To this end, we propose new swapbased representations to improve the robustness of playability, thereby enabling agents to balance game levels more effectively and quickly compared to traditional PCGRL. By analyzing the agent‚Äôs swapping behavior, we can infer which tile types have the most impact on the balance. We validate our approach in the Neural MMO (NMMO) environment in a competitive two-player scenario. In this extended conference paper, we present improved results, explore the applicability of the method to various forms of balancing beyond equal balancing, compare the performance to another search-based approach, and discuss the application of existing fairness metrics to game balancing.},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Preprint of the journal (IEEE Transactions on Games) paper of the same name},
  file = {C:\Users\rini\Zotero\storage\5A4PMYDE\Rupp et al. - 2024 - Simulation-Driven Balancing of Competitive Game Levels with Reinforcement Learning.pdf}
}

@online{SimulationDrivenBalancingCompetitive,
  title = {Simulation-{{Driven Balancing}} of {{Competitive Game Levels}} with {{Reinforcement Learning This}} Research Was Supported by the {{Volkswagen Foundation}} ({{Project}}: {{Consequences}} of {{Artificial Intelligence}} on {{Urban Societies}}, {{Grant}} 98555).},
  url = {https://arxiv.org/html/2503.18748v1},
  urldate = {2025-12-15}
}

@online{specificpark2594AutomaticGameBalancing2023,
  type = {Reddit Post},
  title = {Automatic Game Balancing with {{Reinforcement Learning}}},
  author = {SpecificPark2594},
  date = {2023-02-13T20:52:50},
  url = {https://www.reddit.com/r/gamedesign/comments/111jnyt/automatic_game_balancing_with_reinforcement/},
  urldate = {2025-12-15},
  organization = {r/gamedesign},
  file = {C:\Users\rini\Zotero\storage\5HZHCCMJ\automatic_game_balancing_with_reinforcement.html}
}

@incollection{tanMultiAgentReinforcementLearning1993,
  title = {Multi-{{Agent Reinforcement Learning}}: {{Independent}} vs. {{Cooperative Agents}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1993},
  author = {Tan, Ming},
  date = {1993},
  pages = {330--337},
  publisher = {Elsevier},
  doi = {10.1016/B978-1-55860-307-3.50049-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603073500496},
  urldate = {2025-12-15},
  isbn = {978-1-55860-307-3},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\6UUJM943\Tan - 1993 - Multi-Agent Reinforcement Learning Independent vs. Cooperative Agents.pdf}
}

@online{teamAlphaStarGrandmasterLevel2019,
  title = {{{AlphaStar}}: {{Grandmaster}} Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  shorttitle = {{{AlphaStar}}},
  author = {{team}, The AlphaStar},
  date = {2019-10-30T00:00:00+00:00},
  url = {https://deepmind.google/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/},
  urldate = {2026-01-29},
  abstract = {AlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions. This January, a preliminary version of AlphaStar challenged two of the world's top players in StarCraft II, one of the most enduring and popular real-time strategy video games of all time. Since then, we have taken on a much greater challenge: playing the full game at a Grandmaster level under professionally approved conditions.},
  langid = {english},
  organization = {Google DeepMind},
  file = {C:\Users\rini\Zotero\storage\XWPS74Z7\alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning.html}
}

@article{vinyalsGrandmasterLevelStarCraft2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha√´l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R√©mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W√ºnsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  date = {2019-11},
  journaltitle = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  url = {https://www.nature.com/articles/s41586-019-1724-z},
  urldate = {2026-01-29},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1‚Äì3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players. AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2\% of human players for the real-time strategy game StarCraft II.},
  langid = {english},
  keywords = {Computer science,Statistics}
}

@article{vinyalsGrandmasterLevelStarCraft2019a,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha√´l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R√©mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W√ºnsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  date = {2019-11-14},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  url = {https://www.nature.com/articles/s41586-019-1724-z},
  urldate = {2026-01-29},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\WX2PFBRZ\Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agent reinforcement learning.pdf}
}

@article{yannakakisRealTimeGameAdaptation2009,
  title = {Real-{{Time Game Adaptation}} for {{Optimizing Player Satisfaction}}},
  author = {Yannakakis, G.N. and Hallam, J.},
  date = {2009-06},
  journaltitle = {IEEE Transactions on Computational Intelligence and AI in Games},
  shortjournal = {IEEE Trans. Comput. Intell. AI Games},
  volume = {1},
  number = {2},
  pages = {121--133},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2009.2024533},
  url = {http://ieeexplore.ieee.org/document/5067382/},
  urldate = {2025-12-15},
  abstract = {A methodology for optimizing player satisfaction in games on the ‚ÄúPlayware‚Äù physical interactive platform is demonstrated in this paper. Previously constructed artificial neural network user models, reported in the literature, map individual playing characteristics to reported entertainment preferences for augmented-reality game players. An adaptive mechanism then adjusts controllable game parameters in real time in order to improve the entertainment value of the game for the player. The basic approach presented here applies gradient ascent to the user model to suggest the direction of parameter adjustment that leads toward games of higher entertainment value. A simple rule set exploits the derivative information to adjust specific game parameters to augment the entertainment value. Those adjustments take place frequently during the game with interadjustment intervals that maintain the user model‚Äôs accuracy. Performance of the adaptation mechanism is evaluated using a game survey experiment. Results indicate the efficacy and robustness of the mechanism in adapting the game according to a user‚Äôs individual playing features and enhancing the gameplay experience. The limitations and the use of the methodology as an effective adaptive mechanism for entertainment capture and augmentation are discussed.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\MWW3XPYD\Yannakakis and Hallam - 2009 - Real-Time Game Adaptation for Optimizing Player Satisfaction.pdf}
}

@article{zahariaAcceleratingMachineLearning,
  title = {Accelerating the {{Machine Learning Lifecycle}} with {{MLÔ¨Çow}}},
  author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
  abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\BTYDCD54\Zaharia et al. - Accelerating the Machine Learning Lifecycle with MLÔ¨Çow.pdf}
}

@inproceedings{zhengWujiAutomaticOnline2019,
  title = {Wuji: {{Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning}}},
  shorttitle = {Wuji},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Zheng, Yan and Xie, Xiaofei and Su, Ting and Ma, Lei and Hao, Jianye and Meng, Zhaopeng and Liu, Yang and Shen, Ruimin and Chen, Yingfeng and Fan, Changjie},
  date = {2019-11},
  pages = {772--784},
  publisher = {IEEE},
  location = {San Diego, CA, USA},
  doi = {10.1109/ASE.2019.00077},
  url = {https://ieeexplore.ieee.org/document/8952543/},
  urldate = {2026-01-29},
  abstract = {Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs1, which have been confirmed by the developers, in the commercial games.},
  eventtitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  isbn = {978-1-7281-2508-4},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\YTXSXS5U\Zheng et al. - 2019 - Wuji Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning.pdf}
}
